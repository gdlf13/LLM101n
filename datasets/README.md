# Datasets
- now that we have all these nice performance optimizations, we can start to think about the data. Most importantly how high quality data we can get, and how much of it we can get. Huggingface is a great resource for this: huggingface.co/datasets. They essentially store the best open source datasets in the world for free. You just pip install huggingface datasets and you're good to load any dataset you want. 
- one thing you may notice is this weird `num_proc` term. This is a little hack we use to maximize download speed by downloading multiple files at once. Num procs comes in handy when you have a lot of small files to download. 


# Data/Batch Loading
- from beginner to advanced, you will always run into the problem of a slow dataloader or data pipeline. Ideally you want the bottleneck to be the model you are training or feeding the data to, not the initial dataloading itself.
- we can revisit the term `num_proc` here, as well as `num_workers` in PyTorch. These are essentially the same thing, but `num_proc` is a little more general and can be used in other contexts. By default, you have a single CPU core in charge of executing python code, but you can bump this up to as many cores as you have with `multithreading` or `multiprocessing` (pip libraries). With this alone, you can speedup your data loading by a factor of 4-32x (depending on how nuts your CPU is).
- another trick is to actually load the data in the background while the model is training. This is called `prefetching` and is a common practice in the industry. You can do this with `torch.utils.data.DataLoader` by setting `pin_memory=True` and `num_workers` to the number of CPU cores you have.
- you will see this pattern more obviously as you build and learn more in the space, but you can actually leverage pytorch's blazingly fast internals to automate dataloading with very little external planning. 
- one of my favorite tricks is to load in as much of the dataset I can into system DRAM (not GPU memory). This speeds things up because instead of transferring data from disk to RAM to GPU, you are just transferring from RAM to GPU.
- one the other hand, if you are running a very efficient model on a fairly large piece of hardware (say a raspberry pi spy camera), you can actually load the data directly into the GPU memory before training starts. This way everything you will need for the training runtime is on chip and you don't have to worry about the CPU bottlenecking the GPU.

# Data Cleaning
- data cleaning is one of the more complex and nuanced parts of the deep learning pipeline. You can have the best model in the world, but if you feed it garbage data, it will learn garbage.
- I highly recommend you sift through this [article](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)
- to summarize
    1. scrape a ton of data (commoncrawl = few petabytes (1e15 bytes) of data)
    2. `datatrove` library to filter out obvious noise (adult content, spam, etc.) and deduplicate (same url, same text, etc.)
    3. language filtering depending on if you want only english
    4. filter out synthetic data (generated by other LMs like chatGPT)
    5. using llama3:8b to make sure you have the best of the best educational data (does it pass a threshold of quality?)
- still ongoing research in finding trends across different properties of data, or how the same properties relate across different datasets.

# Synthetic Data Generation
- this is one of the most powerful tools in the deep learning toolbox. You can generate as much data as you want, with as many labels as you want, and as much noise as you want. This is a great way to get around the problem of not having enough data.
- if you were to scrape a bunch of news articles and random text data from the internet, most of it would just be useless noise and your model wouldn't learn quality information. But if you were to use text extraction techniques and filter with cheap models like phi3 or llama3:8b, you could parse out even more of the noise and have your data look so clean it would be hard to tell if a human labeller went through it or not.